{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Titanic Linear Model (grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script type=\"text/javascript\">\n",
       "        \n",
       "        var nb = null;\n",
       "        var kernel = null;\n",
       "\n",
       "        if (IPython && \n",
       "            IPython.notebook && \n",
       "            IPython.notebook.kernel &&\n",
       "            IPython.notebook.kernel.info_reply &&\n",
       "            IPython.notebook.kernel.info_reply.status &&\n",
       "            IPython.notebook.kernel.info_reply.status == \"ok\") {\n",
       "            nb = IPython.notebook; \n",
       "            kernel = IPython.notebook.kernel;\n",
       "        }\n",
       "        \n",
       "        if (nb && kernel) {\n",
       "            var filename = nb.notebook_path;\n",
       "            var basename = filename.substring(filename.lastIndexOf('/') + 1);\n",
       "            var msg = 'Detected filename: '+basename;\n",
       "            document.getElementById(\"detected_notebook_filename_tag\").innerHTML=msg;\n",
       "            var command = \"import os; os.environ['NB_FILENAME']= '\" + basename + \"'\";\n",
       "            kernel.execute(command);\n",
       "        } else {\n",
       "            var msg = 'Not connected to kernel.';\n",
       "            document.getElementById(\"detected_notebook_filename_tag\").innerHTML=msg;\n",
       "        }\n",
       "        </script><pre id=\"detected_notebook_filename_tag\"></pre>\n",
       "      "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%datalabframework getfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-30 14:28:26,053 - jovyan - __main__ - INFO - init - {'project': {'main': 'main.ipynb', 'rootpath': '/home/jovyan/work/notebooks'}, 'datalab': {'framework': '0.1'}, 'notebook': {'filepath': '/home/jovyan/work/notebooks/models', 'filename': 'regression.ipynb'}}\n"
     ]
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "logger = dlf.log.initLogger(__name__, kafka_topic=\"datalab\", kafka_servers=\"kafka:9092\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "\n",
    "def lr_model(df):\n",
    "   \n",
    "    lr = LogisticRegression()\n",
    "    \n",
    "    grid = ParamGridBuilder() \\\n",
    "             .addGrid(lr.maxIter, [1,10,50,150,200,500])\\\n",
    "             .addGrid(lr.regParam, [0.01, 0.05, 0.1,]).build()\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "    cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "    model = cv.fit(df)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = '/home/jovyan/work/data'\n",
    "d = {   \n",
    "    'input_sample' : 1.0,\n",
    "    'input_source' : DATA_ROOT + '/titanic/data/set/train/features.parquet',\n",
    "    'output_model' : DATA_ROOT + '/titanic/models/lr_tune.model'\n",
    "}\n",
    "p = dlf.params.config_fromdict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(dlf.project.filename()).getOrCreate()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+------------------+-----+-----+-------+--------+------+\n",
      "|PassengerId|Survived|Pclass|   Sex|               Age|SibSp|Parch|   Fare|Embarked| Title|\n",
      "+-----------+--------+------+------+------------------+-----+-----+-------+--------+------+\n",
      "|          1|       0|     3|  male|              22.0|    1|    0|   7.25|       S|    Mr|\n",
      "|          2|       1|     1|female|              38.0|    1|    0|71.2833|       C|   Mrs|\n",
      "|          3|       1|     3|female|              26.0|    0|    0|  7.925|       S|  Miss|\n",
      "|          4|       1|     1|female|              35.0|    1|    0|   53.1|       S|   Mrs|\n",
      "|          5|       0|     3|  male|              35.0|    0|    0|   8.05|       S|    Mr|\n",
      "|          6|       0|     3|  male|28.467886947074096|    0|    0| 8.4583|       Q|    Mr|\n",
      "|          7|       0|     1|  male|              54.0|    0|    0|51.8625|       S|    Mr|\n",
      "|          8|       0|     3|  male|               2.0|    3|    1| 21.075|       S|Master|\n",
      "|          9|       1|     3|female|              27.0|    0|    2|11.1333|       S|   Mrs|\n",
      "|         10|       1|     2|female|              14.0|    1|    0|30.0708|       C|   Mrs|\n",
      "|         11|       1|     3|female|               4.0|    1|    1|   16.7|       S|  Miss|\n",
      "|         12|       1|     1|female|              58.0|    0|    0|  26.55|       S|  Miss|\n",
      "|         13|       0|     3|  male|              20.0|    0|    0|   8.05|       S|    Mr|\n",
      "|         14|       0|     3|  male|              39.0|    1|    5| 31.275|       S|    Mr|\n",
      "|         15|       0|     3|female|              14.0|    0|    0| 7.8542|       S|  Miss|\n",
      "|         16|       1|     2|female|              55.0|    0|    0|   16.0|       S|   Mrs|\n",
      "|         17|       0|     3|  male|               2.0|    4|    1| 29.125|       Q|Master|\n",
      "|         18|       1|     2|  male| 34.90209456786358|    0|    0|   13.0|       S|    Mr|\n",
      "|         19|       0|     3|female|              31.0|    1|    0|   18.0|       S|   Mrs|\n",
      "|         20|       1|     3|female|24.042311018333873|    0|    0|  7.225|       C|   Mrs|\n",
      "+-----------+--------+------+------+------------------+-----+-----+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(p.input_source)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/jovyan/work/notebooks/features/features.ipynb\n"
     ]
    }
   ],
   "source": [
    "from features import features\n",
    "df_features = features.feature_vector(df, \n",
    "                  'PassengerId', \n",
    "                  'Survived', \n",
    "                  ['Pclass', \n",
    "                   'Sex', \n",
    "                   'Age', \n",
    "                   'SibSp', \n",
    "                   'Parch',\n",
    "                   'Fare', \n",
    "                   'Embarked', \n",
    "                   'Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = lr_model(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now we can optionally save the fitted pipeline to disk\n",
    "import shutil\n",
    "shutil.rmtree(p.output_model)\n",
    "\n",
    "model.bestModel.save(p.output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------------------+-----+\n",
      "|PassengerId|features                                            |label|\n",
      "+-----------+----------------------------------------------------+-----+\n",
      "|1          |(24,[0,1,2,3,5,6,8],[3.0,1.0,22.0,1.0,7.25,1.0,1.0])|0.0  |\n",
      "|2          |(24,[0,2,3,5,7,10],[1.0,38.0,1.0,71.2833,1.0,1.0])  |1.0  |\n",
      "|3          |(24,[0,2,5,6,9],[3.0,26.0,7.925,1.0,1.0])           |1.0  |\n",
      "|4          |(24,[0,2,3,5,6,10],[1.0,35.0,1.0,53.1,1.0,1.0])     |1.0  |\n",
      "|5          |(24,[0,1,2,5,6,8],[3.0,1.0,35.0,8.05,1.0,1.0])      |0.0  |\n",
      "+-----------+----------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show(n=5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|PassengerId|            features|label|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|          1|(24,[0,1,2,3,5,6,...|  0.0|[2.25429810242301...|[0.90502063632020...|       0.0|\n",
      "|          2|(24,[0,2,3,5,7,10...|  1.0|[-2.3482572308897...|[0.08720439673290...|       1.0|\n",
      "|          3|(24,[0,2,5,6,9],[...|  1.0|[-0.4276992620055...|[0.39467586017300...|       1.0|\n",
      "|          4|(24,[0,2,3,5,6,10...|  1.0|[-1.9225232535820...|[0.12758045488993...|       1.0|\n",
      "|          5|(24,[0,1,2,5,6,8]...|  0.0|[2.11735179401167...|[0.89257827729473...|       0.0|\n",
      "+-----------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(df_features)\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation areaUnderROC : 0.8766630449834364\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "metric = evaluator.evaluate(prediction)\n",
    "metric_name = evaluator.getMetricName()\n",
    "\n",
    "print(\"Evaluation {} : {}\".format(metric_name, metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
