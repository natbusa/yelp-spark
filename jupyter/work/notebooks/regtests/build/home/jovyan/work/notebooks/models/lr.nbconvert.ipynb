{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Titanic Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script type=\"text/javascript\">\n",
       "        \n",
       "        var nb = null;\n",
       "        var kernel = null;\n",
       "\n",
       "        if (IPython && \n",
       "            IPython.notebook && \n",
       "            IPython.notebook.kernel &&\n",
       "            IPython.notebook.kernel.info_reply &&\n",
       "            IPython.notebook.kernel.info_reply.status &&\n",
       "            IPython.notebook.kernel.info_reply.status == \"ok\") {\n",
       "            nb = IPython.notebook; \n",
       "            kernel = IPython.notebook.kernel;\n",
       "        }\n",
       "        \n",
       "        if (nb && kernel) {\n",
       "            var filename = nb.notebook_path;\n",
       "            var basename = filename.substring(filename.lastIndexOf('/') + 1);\n",
       "            var msg = 'Detected filename: '+basename;\n",
       "            document.getElementById(\"detected_notebook_filename_tag\").innerHTML=msg;\n",
       "            var command = \"import os; os.environ['NB_FILENAME']= '\" + basename + \"'\";\n",
       "            kernel.execute(command);\n",
       "        } else {\n",
       "            var msg = 'Not connected to kernel.';\n",
       "            document.getElementById(\"detected_notebook_filename_tag\").innerHTML=msg;\n",
       "        }\n",
       "        </script><pre id=\"detected_notebook_filename_tag\"></pre>\n",
       "      "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%datalabframework getfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-30 14:28:12,715 - jovyan - __main__ - INFO - init - {'datalab': {'framework': '0.1'}, 'notebook': {'filepath': '/home/jovyan/work/notebooks/models', 'filename': 'regression.ipynb'}, 'project': {'main': 'main.ipynb', 'rootpath': '/home/jovyan/work/notebooks'}}\n"
     ]
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "logger = dlf.log.initLogger(__name__, kafka_topic=\"datalab\", kafka_servers=\"kafka:9092\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "\n",
    "def lr_model(df):\n",
    "   \n",
    "    lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "    model = lr.fit(df)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = '/home/jovyan/work/data'\n",
    "d = {   \n",
    "    'input_sample' : 1.0,\n",
    "    'input_source' : DATA_ROOT + '/titanic/data/set/train/features.parquet',\n",
    "    'output_model' : DATA_ROOT + '/titanic/models/lr.model'\n",
    "}\n",
    "p = dlf.params.config_fromdict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(dlf.project.filename()).getOrCreate()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+------------------+-----+-----+-------+--------+------+\n",
      "|PassengerId|Survived|Pclass|Sex   |Age               |SibSp|Parch|Fare   |Embarked|Title |\n",
      "+-----------+--------+------+------+------------------+-----+-----+-------+--------+------+\n",
      "|1          |0       |3     |male  |22.0              |1    |0    |7.25   |S       |Mr    |\n",
      "|2          |1       |1     |female|38.0              |1    |0    |71.2833|C       |Mrs   |\n",
      "|3          |1       |3     |female|26.0              |0    |0    |7.925  |S       |Miss  |\n",
      "|4          |1       |1     |female|35.0              |1    |0    |53.1   |S       |Mrs   |\n",
      "|5          |0       |3     |male  |35.0              |0    |0    |8.05   |S       |Mr    |\n",
      "|6          |0       |3     |male  |28.467886947074096|0    |0    |8.4583 |Q       |Mr    |\n",
      "|7          |0       |1     |male  |54.0              |0    |0    |51.8625|S       |Mr    |\n",
      "|8          |0       |3     |male  |2.0               |3    |1    |21.075 |S       |Master|\n",
      "|9          |1       |3     |female|27.0              |0    |2    |11.1333|S       |Mrs   |\n",
      "|10         |1       |2     |female|14.0              |1    |0    |30.0708|C       |Mrs   |\n",
      "+-----------+--------+------+------+------------------+-----+-----+-------+--------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(p.input_source)\n",
    "df.printSchema()\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/jovyan/work/notebooks/features/features.ipynb\n"
     ]
    }
   ],
   "source": [
    "from features import features\n",
    "df_features = features.feature_vector(df, \n",
    "                  'PassengerId', \n",
    "                  'Survived', \n",
    "                  ['Pclass', \n",
    "                   'Sex', \n",
    "                   'Age', \n",
    "                   'SibSp', \n",
    "                   'Parch',\n",
    "                   'Fare', \n",
    "                   'Embarked', \n",
    "                   'Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = lr_model(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# no overwrite mode in python for spark yet :(\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(p.output_model)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.save(p.output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------------------+-----+\n",
      "|PassengerId|features                                            |label|\n",
      "+-----------+----------------------------------------------------+-----+\n",
      "|1          |(24,[0,1,2,3,5,6,8],[3.0,1.0,22.0,1.0,7.25,1.0,1.0])|0.0  |\n",
      "|2          |(24,[0,2,3,5,7,10],[1.0,38.0,1.0,71.2833,1.0,1.0])  |1.0  |\n",
      "|3          |(24,[0,2,5,6,9],[3.0,26.0,7.925,1.0,1.0])           |1.0  |\n",
      "|4          |(24,[0,2,3,5,6,10],[1.0,35.0,1.0,53.1,1.0,1.0])     |1.0  |\n",
      "|5          |(24,[0,1,2,5,6,8],[3.0,1.0,35.0,8.05,1.0,1.0])      |0.0  |\n",
      "+-----------+----------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show(n=5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|PassengerId|            features|label|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|          1|(24,[0,1,2,3,5,6,...|  0.0|[2.56148487671463...|[0.92834130012375...|       0.0|\n",
      "|          2|(24,[0,2,3,5,7,10...|  1.0|[-2.4922684671590...|[0.07640196973048...|       1.0|\n",
      "|          3|(24,[0,2,5,6,9],[...|  1.0|[-0.8402052862241...|[0.30149155010755...|       1.0|\n",
      "|          4|(24,[0,2,3,5,6,10...|  1.0|[-1.9965592496936...|[0.11956465232543...|       1.0|\n",
      "|          5|(24,[0,1,2,5,6,8]...|  0.0|[2.13531689262064...|[0.89428870306582...|       0.0|\n",
      "+-----------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(df_features)\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation areaUnderROC : 0.8742077567933189\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "metric = evaluator.evaluate(prediction)\n",
    "metric_name = evaluator.getMetricName()\n",
    "\n",
    "print(\"Evaluation {} : {}\".format(metric_name, metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
