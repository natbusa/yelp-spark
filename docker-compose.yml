version: '2'
services:
  # cassandra:
  #   image: cassandra
  #   volumes:
  #     - ./volumes/cassandra:/var/lib/cassandra

  notebook:
    build: jupyter
    command: start.sh jupyter notebook
    volumes:
      - ./jupyter/work:/home/jovyan/work
    environment:
      SPARK_OPTS: '--master=spark://spark-master:7077'
    ports:
      - 8888:8888

  zookeeper:
    image: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zookeeper:2888:3888
  
  kafka:
    image: wurstmeister/kafka
    ports:
      - 9092:9092
    environment:
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "datalab:1:1,test:1:1,events:1:1"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

  # spark-master:
  #   image: jupyter/pyspark-notebook
  #   command: /usr/local/spark/bin/spark-class org.apache.spark.deploy.master.Master -h spark-master
  #   hostname: spark-master
  #   environment:
  #     MASTER: spark://spark-master:7077
  #     SPARK_CONF_DIR: /conf
  #     SPARK_PUBLIC_DNS: localhost
  #   ports:
  #     - 4040
  #     - 6066
  #     - 7001
  #     - 7002
  #     - 7003
  #     - 7004
  #     - 7005
  #     - 7006
  #     - 7077
  #     - 8080:8080
  #   volumes:
  #     - ./spark/conf/master:/conf
  #     - ./volumes/spark/master/data:/tmp/data

  # spark-worker:
  #   image: jupyter/pyspark-notebook
  #   command: /usr/local/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  #   hostname: spark-worker
  #   environment:
  #     SPARK_CONF_DIR: /conf
  #     SPARK_WORKER_CORES: 2
  #     SPARK_WORKER_MEMORY: 1g
  #     SPARK_WORKER_PORT: 8881
  #     SPARK_WORKER_WEBUI_PORT: 8081
  #     SPARK_PUBLIC_DNS: localhost
  #   ports:
  #     - 7012
  #     - 7013
  #     - 7014
  #     - 7015
  #     - 7016
  #     - 8081:8081
  #   volumes:
  #     - ./spark/conf/worker:/conf
  #     - ./volumes/spark/worker/data:/tmp/data
  #     - ./volumes/spark/worker/work:/usr/local/spark/work

  # hdfs-nn:
  #   image: itrust/hdfs:2.7.1
  #   hostname: hdfs-nn
  #   command: /run-namenode.sh
  #   volumes:
  #     - ./volumes/hdfs/namenode:/hadoop/dfs/name
  #   environment:
  #     - CLUSTER_NAME=test
  #     - CORE_CONF_fs_defaultFS=hdfs://hdfs-nn:8020
  #     - CORE_CONF_hadoop_http_staticuser_user=root
  #     - CORE_CONF_hadoop_proxyuser_hue_hosts=*
  #     - CORE_CONF_hadoop_proxyuser_hue_groups=*
  #     - HDFS_CONF_dfs_webhdfs_enabled=true
  #     - HDFS_CONF_dfs_permissions_enabled=false
  #   ports:
  #     - 50070:50070

  # hdfs-dn:
  #   image: itrust/hdfs:2.7.1
  #   links:
  #       - hdfs-nn
  #   command: /run-datanode.sh
  #   volumes:
  #     - ./volumes/hdfs/datanode:/hadoop/dfs/data
  #   environment:
  #     - CORE_CONF_fs_defaultFS=hdfs://hdfs-nn:8020
  #     - CORE_CONF_hadoop_http_staticuser_user=root
  #     - CORE_CONF_hadoop_proxyuser_hue_hosts=*
  #     - CORE_CONF_hadoop_proxyuser_hue_groups=*
  #     - HDFS_CONF_dfs_webhdfs_enabled=true
  #     - HDFS_CONF_dfs_permissions_enabled=false
